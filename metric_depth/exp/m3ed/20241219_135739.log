/home/liangao/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
[2024-12-19 13:57:41,224] torch.distributed.run: [WARNING] 
[2024-12-19 13:57:41,224] torch.distributed.run: [WARNING] *****************************************
[2024-12-19 13:57:41,224] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-12-19 13:57:41,224] torch.distributed.run: [WARNING] *****************************************
xFormers not available
xFormers not available
xFormers not available
xFormers not available
[2024-12-19 13:57:44.721] [ouster::sensor] [info] parsing legacy metadata format
[2024-12-19 13:57:44,728][    INFO] {'bs': 4,
 'dataset': 'm3ed',
 'encoder': 'vits',
 'epochs': 120,
 'img_size': 518,
 'local_rank': 0,
 'lr': 5e-06,
 'max_depth': 90.0,
 'min_depth': 0.001,
 'ngpus': 2,
 'port': 20596,
 'pretrained_from': './depth_anything_v2_vits.pth',
 'save_path': 'exp/m3ed'}

[2024-12-19 13:57:44.731] [ouster::sensor] [info] parsing legacy metadata format
[2024-12-19 13:57:44.749] [ouster::sensor] [info] parsing legacy metadata format
[2024-12-19 13:57:44.755] [ouster::sensor] [info] parsing legacy metadata format
[2024-12-19 13:57:45,989][    INFO] ===========> Epoch: 0/120, d1: 0.000, d2: 0.000, d3: 0.000
[2024-12-19 13:57:45,989][    INFO] ===========> Epoch: 0/120, abs_rel: 100.000, sq_rel: 100.000, rmse: 100.000, rmse_log: 100.000, log10: 100.000, silog: 100.000
/home/liangao/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [1, 32, 1, 1], strides() = [32, 1, 32, 32]
bucket_view.sizes() = [1, 32, 1, 1], strides() = [32, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/liangao/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [1, 32, 1, 1], strides() = [32, 1, 32, 32]
bucket_view.sizes() = [1, 32, 1, 1], strides() = [32, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Traceback (most recent call last):
  File "/data1/liangao/AlanLiang/Projects/MVLM/Depth-Anything-V2-M3ED/metric_depth/train.py", line 219, in <module>
    main()
  File "/data1/liangao/AlanLiang/Projects/MVLM/Depth-Anything-V2-M3ED/metric_depth/train.py", line 142, in main
    loss.backward()
  File "/home/liangao/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/home/liangao/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.55 GiB. GPU 0 has a total capacty of 44.35 GiB of which 2.37 GiB is free. Including non-PyTorch memory, this process has 41.97 GiB memory in use. Of the allocated memory 39.74 GiB is allocated by PyTorch, and 1.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/data1/liangao/AlanLiang/Projects/MVLM/Depth-Anything-V2-M3ED/metric_depth/train.py", line 219, in <module>
    main()
  File "/data1/liangao/AlanLiang/Projects/MVLM/Depth-Anything-V2-M3ED/metric_depth/train.py", line 142, in main
    loss.backward()
  File "/home/liangao/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/home/liangao/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.55 GiB. GPU 1 has a total capacty of 44.35 GiB of which 17.81 MiB is free. Including non-PyTorch memory, this process has 44.32 GiB memory in use. Of the allocated memory 42.28 GiB is allocated by PyTorch, and 1.54 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[2024-12-19 13:57:56,265] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 3364316) of binary: /home/liangao/miniconda3/envs/openmmlab/bin/python3
Traceback (most recent call last):
  File "/home/liangao/miniconda3/envs/openmmlab/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/liangao/miniconda3/envs/openmmlab/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/liangao/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/distributed/launch.py", line 196, in <module>
    main()
  File "/home/liangao/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/distributed/launch.py", line 192, in main
    launch(args)
  File "/home/liangao/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/distributed/launch.py", line 177, in launch
    run(args)
  File "/home/liangao/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/home/liangao/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/liangao/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-12-19_13:57:56
  host      : server
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 3364317)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-12-19_13:57:56
  host      : server
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3364316)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
